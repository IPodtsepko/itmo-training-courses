{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Построение модели (подбор алгоритма и гиперпараметров)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from src.config import TARGET_COLUMN\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/preprocessed/train.csv')\n",
    "\n",
    "features = df.drop(columns=[TARGET_COLUMN])\n",
    "target = df[TARGET_COLUMN]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Будем использовать кросс-валидацию как функцию ошибки при подборе гиперпараметров."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def cross_validation(classifier) -> float:\n",
    "    error = 0\n",
    "    for train, test in KFold(n_splits=5, shuffle=True).split(features, target):\n",
    "        classifier.fit(features.loc[train], target.loc[train])\n",
    "        classifier_prediction = classifier.predict(features.loc[test])\n",
    "        error += mean_squared_error(target.loc[test], classifier_prediction)\n",
    "    return error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Я бы очень хотел написать функцию `objective` следующим образом:\n",
    "\n",
    "```python\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    algorithm = trial.suggest_categorical('algorithm', ['SVC',\n",
    "                                                        'DecisionTreeClassifier',\n",
    "                                                        'KNeighborsClassifier',\n",
    "                                                        'GaussianNB',\n",
    "                                                        'AdaBoostClassifier'])\n",
    "    match algorithm:\n",
    "        case 'SVC':\n",
    "            svc_c = trial.suggest_float('svc_c', 0.01, 10.0)\n",
    "            kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "            degree = 3\n",
    "            if kernel == 'poly':\n",
    "                degree = trial.suggest_int('degree', 1, 5)\n",
    "            gamma = 'scale'\n",
    "            if kernel in ['rbf', 'poly', 'sigmoid']:\n",
    "                gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "            coef0 = 0.0\n",
    "            if kernel in ['poly', 'sigmoid']:\n",
    "                coef0 = trial.suggest_float('coef0', -1, 1)\n",
    "            shrinking = trial.suggest_categorical('shrinking', [False, True])\n",
    "            probability = trial.suggest_categorical('probability', [False, True])\n",
    "            classifier = SVC(C=svc_c,\n",
    "                             kernel=kernel,\n",
    "                             degree=degree,\n",
    "                             gamma=gamma,\n",
    "                             coef0=coef0,\n",
    "                             shrinking=shrinking,\n",
    "                             probability=probability)\n",
    "        case 'DecisionTreeClassifier':\n",
    "            criterion = trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss'])\n",
    "            splitter = trial.suggest_categorical('splitter', ['best', 'random'])\n",
    "            max_depth = None\n",
    "            if trial.suggest_categorical('use max_depth', [False, True]):\n",
    "                max_depth = trial.suggest_int('max_depth', 1, 100)\n",
    "            classifier = DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_depth=max_depth)\n",
    "        case 'KNeighborsClassifier':\n",
    "            n_neighbors = trial.suggest_int('n_neighbors', 1, 100)\n",
    "            weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "            metric = trial.suggest_categorical('metric', ['cityblock',\n",
    "                                                          'cosine',\n",
    "                                                          'euclidean',\n",
    "                                                          'l1',\n",
    "                                                          'l2',\n",
    "                                                          'manhattan',\n",
    "                                                          'nan_euclidean'])\n",
    "            classifier = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=metric)\n",
    "        case 'GaussianNB':\n",
    "            priors = None\n",
    "            if trial.suggest_categorical('use prior', [False, True]):\n",
    "                prior = trial.suggest_float('prior', 0.0, 1.0)\n",
    "                priors = [prior, 1.0 - prior]\n",
    "            classifier = GaussianNB(priors=priors)\n",
    "        case 'AdaBoostClassifier':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 1, 150)\n",
    "            learning_rate = trial.suggest_float('learning_rate', 0.001, 5.0)\n",
    "            classifier = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "        case _:\n",
    "            raise ValueError(f'Unexpected algorithm: {algorithm}')\n",
    "    return cross_validation(classifier=classifier)\n",
    "```\n",
    "\n",
    "Однако в силу того, что в задании требуется вывести информацию о полезности гиперпараметров - придется вынести все вызовы `suggest_` на верхний уровень."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, get_classifier: bool = False):\n",
    "    algorithm = trial.suggest_categorical('algorithm', ['SVC',\n",
    "                                                        'DecisionTreeClassifier',\n",
    "                                                        'KNeighborsClassifier',\n",
    "                                                        'GaussianNB',\n",
    "                                                        'AdaBoostClassifier'])\n",
    "\n",
    "    # Hyperparameters for SVC\n",
    "    svc_c = trial.suggest_float('svc_c', 0.01, 10.0)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "    degree = 3\n",
    "    if kernel == 'poly':\n",
    "        degree = trial.suggest_int('degree', 1, 5)\n",
    "    gamma = 'scale'\n",
    "    if kernel in ['rbf', 'poly', 'sigmoid']:\n",
    "        gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    coef0 = 0.0\n",
    "    if kernel in ['poly', 'sigmoid']:\n",
    "        coef0 = trial.suggest_float('coef0', -1, 1)\n",
    "    shrinking = trial.suggest_categorical('shrinking', [False, True])\n",
    "    probability = trial.suggest_categorical('probability', [False, True])\n",
    "\n",
    "    # Hyperparameters for DecisionTreeClassifier\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss'])\n",
    "    splitter = trial.suggest_categorical('splitter', ['best', 'random'])\n",
    "    max_depth = None\n",
    "    if trial.suggest_categorical('use max_depth', [False, True]):\n",
    "        max_depth = trial.suggest_int('max_depth', 1, 100)\n",
    "\n",
    "    # Hyperparameters for KNeighborsClassifier\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 100)\n",
    "    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "    metric = trial.suggest_categorical('metric', ['cityblock',\n",
    "                                                  'cosine',\n",
    "                                                  'euclidean',\n",
    "                                                  'l1',\n",
    "                                                  'l2',\n",
    "                                                  'manhattan',\n",
    "                                                  'nan_euclidean'])\n",
    "\n",
    "    # Hyperparameters for GaussianNB\n",
    "    priors = None\n",
    "    if trial.suggest_categorical('use prior', [False, True]):\n",
    "        prior = trial.suggest_float('prior', 0.0, 1.0)\n",
    "        priors = [prior, 1.0 - prior]\n",
    "\n",
    "    # Hyperparameters for AdaBoostClassifier\n",
    "    n_estimators = trial.suggest_int('n_estimators', 1, 150)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.001, 5.0)\n",
    "\n",
    "    match algorithm:\n",
    "        case 'SVC':\n",
    "            classifier = SVC(C=svc_c,\n",
    "                             kernel=kernel,\n",
    "                             degree=degree,\n",
    "                             gamma=gamma,\n",
    "                             coef0=coef0,\n",
    "                             shrinking=shrinking,\n",
    "                             probability=probability)\n",
    "        case 'DecisionTreeClassifier':\n",
    "            classifier = DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_depth=max_depth)\n",
    "        case 'KNeighborsClassifier':\n",
    "            classifier = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=metric)\n",
    "        case 'GaussianNB':\n",
    "            classifier = GaussianNB(priors=priors)\n",
    "        case 'AdaBoostClassifier':\n",
    "            classifier = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "        case _:\n",
    "            raise ValueError(f'Unexpected algorithm: {algorithm}')\n",
    "\n",
    "    if get_classifier:\n",
    "        return classifier\n",
    "\n",
    "    return cross_validation(classifier=classifier)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Первый запуск выполним с базовым sampler'ом, установим число итераций равное 100 и посмотрим на результат оптимизации."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for BaseSampler: 0.11068943706514864\n",
      "Best params for BaseSampler: {'algorithm': 'SVC', 'svc_c': 7.452279016078824, 'kernel': 'rbf', 'gamma': 'auto', 'shrinking': False, 'probability': False, 'criterion': 'entropy', 'splitter': 'random', 'use max_depth': True, 'max_depth': 24, 'n_neighbors': 28, 'weights': 'distance', 'metric': 'nan_euclidean', 'use prior': False, 'n_estimators': 134, 'learning_rate': 0.4046155642063821}\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "study_with_base_sampler = optuna.create_study(study_name=\"Study with BaseSampler\")\n",
    "study_with_base_sampler.optimize(objective, n_trials=100);\n",
    "\n",
    "best_trial_for_base_sampler = study_with_base_sampler.best_trial\n",
    "print(f'Best score for BaseSampler: {best_trial_for_base_sampler.value}')\n",
    "print(f'Best params for BaseSampler: {best_trial_for_base_sampler.params}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Отобразим в браузере график зависимости функции ошибки от номера итерации."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study_with_base_sampler).show(renderer='browser')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "А также информацию о полезности гиперпараметров."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study_with_base_sampler).show(renderer='browser');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для сравнения выполним запуск с использованием RandomSampler с тем же числом итераций."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for RandomSampler(seed=17): 0.10651132823395838\n",
      "Best params for RandomSampler(seed=17): {'algorithm': 'SVC', 'svc_c': 8.571532161838777, 'kernel': 'rbf', 'gamma': 'auto', 'shrinking': False, 'probability': False, 'criterion': 'gini', 'splitter': 'best', 'use max_depth': False, 'n_neighbors': 92, 'weights': 'uniform', 'metric': 'l1', 'use prior': False, 'n_estimators': 41, 'learning_rate': 0.6289061699686381}\n"
     ]
    }
   ],
   "source": [
    "study_with_random_sampler = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=17),\n",
    "                                                study_name=\"Study with RandomSampler(seed=17)\")\n",
    "study_with_random_sampler.optimize(objective, n_trials=100);\n",
    "\n",
    "best_trial_for_random_sampler = study_with_random_sampler.best_trial\n",
    "print(f'Best score for RandomSampler(seed=17): {best_trial_for_random_sampler.value}')\n",
    "print(f'Best params for RandomSampler(seed=17): {best_trial_for_random_sampler.params}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "И снова построим необходимые графики."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study_with_random_sampler).show(renderer='browser')\n",
    "optuna.visualization.plot_param_importances(study_with_random_sampler).show(renderer='browser');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь сравним результаты двух лучших моделей на тестовой выборке."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result of BaseSampler: 0.049939098660170524\n",
      "Final result of RandomSampler(seed=17): 0.05115712545676005\n"
     ]
    }
   ],
   "source": [
    "from src.checker import mean_squared_error_of\n",
    "\n",
    "test_df = pd.read_csv('./data/preprocessed/test.csv')\n",
    "\n",
    "def predict_for_trial(trial: optuna.trial.Trial) -> pd.DataFrame:\n",
    "    best_classifier = objective(trial, get_classifier=True)\n",
    "    best_classifier.fit(features, target)\n",
    "    return pd.DataFrame(best_classifier.predict(test_df), columns=[TARGET_COLUMN])\n",
    "\n",
    "prediction_for_base_sampler = predict_for_trial(best_trial_for_base_sampler)\n",
    "print(f'Final result of BaseSampler: {mean_squared_error_of(prediction_for_base_sampler)}')\n",
    "\n",
    "prediction_for_random_sampler = predict_for_trial(best_trial_for_random_sampler)\n",
    "print(f'Final result of RandomSampler(seed=17): {mean_squared_error_of(prediction_for_random_sampler)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вывод: здесь только словами, так как в силу того, что подбор не является детерминированным - иногда результаты могут отличаться. Но для оценки результата можно посмотреть на base line - в данном случае предсказание класса нулями/единицами независимо от признаков."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base line with 0: 0.09622411693057248\n",
      "Base line with 1: 0.9037758830694276\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f'Base line with 0: {mean_squared_error_of(pd.DataFrame(np.zeros(1642), columns=[TARGET_COLUMN]))}')\n",
    "print(f'Base line with 1: {mean_squared_error_of(pd.DataFrame(np.zeros(1642) + 1, columns=[TARGET_COLUMN]))}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Лабораторная №3. Обработка последовательностей\n",
    "\n",
    "**Выполнил**: Подцепко Игорь, учeбная группа M33351\n",
    "\n",
    "## Предобработка текста"
   ],
   "metadata": {
    "id": "96w31AcQ8pe-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict\n",
    "from typing import Union"
   ],
   "metadata": {
    "id": "ubzRxxxWM9S3"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Config:\n",
    "    FILE_PATH: str = \"data/moris.txt\"\n",
    "    STOP_CHAR: str = \".\"\n",
    "    PREDICTION_LENGTH_LIMIT: int = 50\n",
    "    EXAMPLE_PREFIX: str = \"морис ска\"\n",
    "    BATCH_SIZE: int = len(EXAMPLE_PREFIX)"
   ],
   "metadata": {
    "id": "IzStI6ljKgND"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Поскольку для предобработки текста я собираюсь испольовать библиотеку `nltk` - необходимо кое-что загрузить:"
   ],
   "metadata": {
    "id": "nBUDmVsk9Lvh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download(\"punkt\");"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWPie6F6Q5Er",
    "outputId": "07d29c00-4bff-4c91-88bb-bb703940f1fc"
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/igor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def is_russian(word: str) -> bool:\n",
    "    \"\"\" Allows only words consisting of Russian letters \"\"\"\n",
    "    pattern = re.compile(\"^[а-яА-Я]+$\")\n",
    "    return pattern.search(word) is not None\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \" \".join([word for word in word_tokenize(sentence) if is_russian(word)]) + '.'\n",
    "    for sentence in sent_tokenize(Path(Config.FILE_PATH).read_text().lower().replace(\"ё\", \"е\"))\n",
    "]\n",
    "text = \" \".join(sentences)\n",
    "print(f'Found {len(sentences)} sentences, the length of the text is {len(text)} characters')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3JGHG_JO90S",
    "outputId": "92d00276-1276-4248-bd56-644ff39bd557"
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6527 sentences, the length of the text is 308794 characters\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Модель LSTM\n",
    "\n",
    "Далее закодируем все предложения для работы с моделью LSTM."
   ],
   "metadata": {
    "id": "w8WtTIRG9lec"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "alphabet = sorted(list(set(text)))\n",
    "alphabet_size = len(alphabet)\n",
    "\n",
    "print(f\"Alphabet: '{''.join(alphabet)}' (size {alphabet_size})\")\n",
    "\n",
    "codes = {symbol: code for (code, symbol) in enumerate(alphabet)}\n",
    "\n",
    "\n",
    "def encode(char: str) -> list[int]:\n",
    "    code = codes[char]\n",
    "    return [1 if i == code else 0 for i in range(alphabet_size)]\n",
    "\n",
    "print(\"Encoding:\")\n",
    "encoded = [[encode(char) for char in sentence] for sentence in tqdm(sentences)]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFcxcP9iRVgA",
    "outputId": "1776cf32-bd2b-47a4-c7f8-0951f73f9f81"
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet: ' .абвгдежзийклмнопрстуфхцчшщъыьэюя' (size 34)\n",
      "Encoding:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6527/6527 [00:00<00:00, 7845.75it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Подготовим данные для обучения разделив предложения с помощью \"скользящих окон\" фиксированной длины (например, предложение \"морис сказал\" и окна длины 5 будем создавать пары вида (\"мори\", \"c\"), (\"орис\", \" \") и так далее). На этих данных будем обучать модель LSTM ниже."
   ],
   "metadata": {
    "id": "6iDYnMpU9ul5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def split_dataset(encoded_sentences, batch_size) -> tuple[np.ndarray, np.ndarray]:\n",
    "    prefixes = []\n",
    "    continuations = []\n",
    "    print(\"Split dataset to prefixes and continuations:\")\n",
    "    for sentence in tqdm(encoded_sentences):\n",
    "        batch_count = len(sentence) - batch_size\n",
    "        if batch_count <= 0:\n",
    "            continue  # is too small\n",
    "        for i in range(batch_count):\n",
    "            prefixes.append(sentence[i: i + batch_size])\n",
    "            continuations.append(sentence[i + batch_size])\n",
    "    return np.array(prefixes), np.array(continuations)"
   ],
   "metadata": {
    "id": "qeOaHGO9RX7M"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "phrase_prefixes, phrase_continuations = split_dataset(encoded, batch_size=Config.BATCH_SIZE)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7n1UY4uRY1_",
    "outputId": "99042f99-7b43-4d05-b271-882063a0f47e"
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split dataset to prefixes and continuations:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6527/6527 [00:00<00:00, 16059.57it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Следующая вспомогательная функция загружает модель LSTM, если она уже была обучена, или создает и обучает новую."
   ],
   "metadata": {
    "id": "4T80_eko-UnK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def load_lstm(\n",
    "    prefixes: np.ndarray,\n",
    "    continuations: np.ndarray,\n",
    "    units: int,\n",
    "    batch_size: int,\n",
    "    alphabet_size: int,\n",
    "    dropout: float,\n",
    "    dense_activation: str,\n",
    "    loss: str,\n",
    "    optimizer: str,\n",
    "    epoch_count: int,\n",
    "    saving_path: Union[str, None] = None,\n",
    "):\n",
    "    \"\"\"Creates and trains an LSTM model\"\"\"\n",
    "\n",
    "    if saving_path is not None and os.path.exists(saving_path):\n",
    "        return tf.keras.models.load_model(saving_path)\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.LSTM(\n",
    "            units, input_shape=(batch_size, alphabet_size), dropout=dropout\n",
    "        )\n",
    "    )  # LSTM cell\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(alphabet_size, activation=dense_activation)\n",
    "    )  # Just your regular densely-connected NN layer.\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    model.fit(prefixes, continuations, epochs=epoch_count)\n",
    "\n",
    "    if saving_path is not None:\n",
    "        model.save(saving_path)\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "id": "TERX-IUU21d3"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "С помощью вышеупомянутой функции получим модель LSTM:"
   ],
   "metadata": {
    "id": "-21d30E3-hyJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello + [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"hello +\", tf.config.list_physical_devices())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'; import tensorflow as tf; print('Num GPUs Available: ', len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lstm = load_lstm(\n",
    "    prefixes=phrase_prefixes,\n",
    "    continuations=phrase_continuations,\n",
    "    units=128, # Positive integer, dimensionality of the output space\n",
    "    batch_size = Config.BATCH_SIZE,\n",
    "    alphabet_size = alphabet_size,\n",
    "    dropout = 0.2, # Fraction of the units to drop for the linear transformation of the inputs\n",
    "    dense_activation=\"sigmoid\", \n",
    "    loss=\"categorical_crossentropy\", # for example, MSE or CrossEntropy\n",
    "    optimizer=\"adam\", \n",
    "    epoch_count=6,\n",
    "    # saving_path=\"/drive/MyDrive/machine-learning/the-amazing-maurice-lstm.ps\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u96cMxYoRb75",
    "outputId": "6c3ec608-4458-4c76-e48d-b14dd4fb217e"
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3828/7649 [==============>...............] - ETA: 27s - loss: 2.6052"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 19:46:53.824272: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-22 19:46:53.825590: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-22 19:46:53.826484: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(\n",
    "    tf.keras.layers.LSTM(\n",
    "        128, input_shape=(Config.BATCH_SIZE, alphabet_size), dropout=0.1\n",
    "    )\n",
    ")  # LSTM cell\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Напишем также вспомогательную функцию, которая поможет предсказывать целые предложения с помощью обученной модели (а не по одному символу)."
   ],
   "metadata": {
    "id": "5fhZ1MK3-8f_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "\n",
    "def predict_sentence(\n",
    "    model,\n",
    "    prefix,\n",
    "    end_char: str = Config.STOP_CHAR,\n",
    "    prediction_length_limit: int = Config.PREDICTION_LENGTH_LIMIT,\n",
    ") -> str:\n",
    "    \"\"\"Predicts a sentence by prefix\"\"\"\n",
    "    window = [encode(char) for char in prefix]\n",
    "    while len(prefix) < prediction_length_limit:\n",
    "        (prediction,) = model.predict(np.array([window]), verbose=0)\n",
    "        prediction = alphabet[np.argmax(prediction)]\n",
    "        prefix += prediction\n",
    "        if prediction == end_char:\n",
    "            break\n",
    "        window = window[1:] + [encode(prediction)]\n",
    "    return prefix\n"
   ],
   "metadata": {
    "id": "248x3NOqRtNA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"LSTM prediction: {predict_sentence(lstm, prefix=Config.EXAMPLE_PREFIX)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UmyGmMt-_hRy",
    "outputId": "c71dadda-dafa-4dbd-886f-f98f66a68eec"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LSTM prediction: морис сказал он.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Марковская цепь\n",
    "\n",
    "В следующей ячейке реализована Марковская цепь (реализация тривиальна)."
   ],
   "metadata": {
    "id": "GCIfnT9h_jwO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MarkovChain:\n",
    "    \"\"\" A class implementing a Markov chain \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int, end_char: str = Config.STOP_CHAR, default_prediction_length_limit: int = Config.PREDICTION_LENGTH_LIMIT):\n",
    "        \"\"\" Сreates an untrained Markov chain \"\"\"\n",
    "        self.transitions = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        # transitions['префик']['с'] - кол-во вхождений 'префикс'\n",
    "        self.batch_size = batch_size\n",
    "        self.end_char = end_char\n",
    "        self.default_prediction_length_limit = default_prediction_length_limit\n",
    "\n",
    "    def fit(self, data: str):\n",
    "        \"\"\" Fits the Markov chain on the provided text \"\"\"\n",
    "        for i in range(len(data) - self.batch_size - 1):\n",
    "            self._fit(data[i : i + self.batch_size + 1])\n",
    "    \n",
    "    def _fit(self, data: str):\n",
    "        \"\"\" Accepts a string of size batch_size + 1 and saves statistics\n",
    "        to the frequency dictionary information about the transition to\n",
    "        the last symbol \"\"\"\n",
    "        self.transitions[data[:-1]][data[-1]] += 1\n",
    "\n",
    "    def predict(self, prefix: str, prediction_length_limit: Union[int, None] = None) -> str:\n",
    "        \"\"\" Predict sentency by prefix \"\"\"\n",
    "        prediction = prefix\n",
    "        prefix = prefix[-self.batch_size:]\n",
    "        if prediction_length_limit is None:\n",
    "            prediction_length_limit = self.default_prediction_length_limit\n",
    "        while len(prediction) < prediction_length_limit:\n",
    "            predicted_continuation = self._predict(prefix)\n",
    "            prediction += predicted_continuation\n",
    "            if predicted_continuation == self.end_char:\n",
    "                break\n",
    "            prefix = prefix[1:] + predicted_continuation\n",
    "        return prediction\n",
    "\n",
    "    def _predict(self, prefix: str) -> str:\n",
    "        \"\"\" Predict continuation (1 char) by prefix \"\"\"\n",
    "        if prefix not in self.transitions:\n",
    "            return self.end_char\n",
    "        best_transition, _ = max(self.transitions[prefix].items(), key=lambda item: item[1])\n",
    "        return best_transition"
   ],
   "metadata": {
    "id": "DX2TmSzC5fCf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создаем и обучаем марковскую цепь на той же книге \"Удивительный Морис\"."
   ],
   "metadata": {
    "id": "OHnbZwve_vKc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "markov_chain = MarkovChain(batch_size=Config.BATCH_SIZE)\n",
    "markov_chain.fit(data=text)"
   ],
   "metadata": {
    "id": "7ZiMaQ2UEogu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Markov chain prediction: {markov_chain.predict(prefix=Config.EXAMPLE_PREFIX)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T4JOMYzo_Eiv",
    "outputId": "dcbae954-bb91-44e8-94d3-2e3f6861618f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Markov chain prediction: морис сказал сардины.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Финальное сравнение моделей"
   ],
   "metadata": {
    "id": "epzYcJWkVyW8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for sample in (\n",
    "    \"сказал морис\",\n",
    "    \"крысолов номер\",\n",
    "    \"опасный боб\",\n",
    "    \"персик поднесла\",\n",
    "    \"подумал морис\",\n",
    "    \"морис подумал\",\n",
    "    \"бургомистр\",\n",
    "    \"странно поду\",\n",
    "    \"мельком взглянув на\",\n",
    "    \"машинное обучение\",\n",
    "):\n",
    "    print(\n",
    "        f'\"{predict_sentence(lstm, prefix=sample)}\" vs \"{markov_chain.predict(prefix=sample)}\"'\n",
    "    )\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpMa9etWRUvL",
    "outputId": "d2ed2c67-aaec-4403-aa8d-ae24487a18cd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"сказал морис.\" vs \"сказал морис.\"\n",
      "\"крысолов номер они после тебе посмотрел на сторону\" vs \"крысолов номер один повернулся к персик которая по\"\n",
      "\"опасный боб.\" vs \"опасный боб.\"\n",
      "\"персик поднесла как будто то они была бы они была \" vs \"персик поднесла спичку к свече.\"\n",
      "\"подумал морис.\" vs \"подумал морис.\"\n",
      "\"морис подумал морис.\" vs \"морис подумал морис.\"\n",
      "\"бургомистр подомно ответил мальчик.\" vs \"бургомистр.\"\n",
      "\"странно подумал морис.\" vs \"странно подумал морис.\"\n",
      "\"мельком взглянув на мориса.\" vs \"мельком взглянув на мориса.\"\n",
      "\"машинное обучение подумал морис.\" vs \"машинное обучение.\"\n"
     ]
    }
   ]
  }
 ]
}
